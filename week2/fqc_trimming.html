<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Sean Harrington &amp; Vikram Chhatre" />
  <meta name="dcterms.date" content="2021-11-05" />
  <title>Quality assessment and trimming</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="avenir-white.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Quality assessment and trimming</h1>
<p class="author">Sean Harrington &amp; Vikram Chhatre</p>
<p class="date">November 5, 2021</p>
</header>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><p><a href="#examining-read-quality">1. Examining read quality</a></p></li>
<li><p><a href="#trimming-and-quality-filtering">2. Trimming and quality filtering</a></p></li>
<li><p><a href="#post-trimming-quality-check">3. Post-trimming quality check</a></p></li>
<li><p><a href="#trimming-using-job-arrays">4. Trimming using job arrays</a></p></li>
</ul>
<p><br><br><br><br><br><br> <br><br><br><br><br><br> <br><br><br><br><br><br> <br><br><br><br><br><br> <br><br><br><br><br><br> <br><br><br><br><br><br> <br><br><br><br><br><br> <br><br><br><br><br><br> <br><br><br><br><br><br></p>
<h2 id="examining-read-quality">1. Examining read quality</h2>
<p>Once we have demultiplexed the data, we will want to check the quality of the reads, and check for the presence of Illumina adapters. You may also wish to do this on the combined reads before demultiplexing, however, we know that we have barcodes on each read that will need to be removed in the demultiplexing step, and so we won’t do that here. If we see very poor quality in these demultiplexed FASTQ files, then we would want to go back and check on the quality of the combined FASTQ to determine if the issue started with our raw data or some weird error in demultiplexing.</p>
<p>To check the quality of our reads, we’ll use a tool FASTQC. This runs very fast and is very simple, and we’re going to be waiting on the results anyways, so we’ll run it interactively here, and on only a subset of the FASTQ files so that it will be very fast. If running FASTQC on many files, you may want to write a slurm script to submit this as a job instead.</p>
<p>Let’s start by creating a directory with only a subset of the demultiplexed FASTQ files</p>
<pre><code>cd &lt;PATH_TO_DIRECTORY_CONTAINING_DEMUXED_FASTQs&gt;
mkdir demux_subset
cp SD_Field_1453.fastq.gz SD_Field_12* SD_Field_0506.fastq.gz demux_subset
cd demux_subset</code></pre>
<p>Then we can start up an interactive session and run fastqc on all FASTQ files in this subset directory.</p>
<pre><code>salloc --account=&lt;YOUR_ACCOUNT&gt;  -t 0-1:00:00 --mem=4G --nodes=1 --ntasks-per-node=6
module load swset gcc fastqc/0.11.7 # load the modules necessary
mkdir fastqc_out
fastqc -t 6 *.fastq.gz -o fastqc_out</code></pre>
<p>The <code>-t</code> argument for fastqc tells it how many cores to use. We’ll use multiple since we’re running it on multiple files. The next argument, and the only requirement at all, is a fastq file or list of files. Here we provide it a list of files by using the <code>*</code> wildcard to tell it to run on all files in the current directory that end with <code>fastq.</code>. We have also used the <code>-o</code> option to tell fastqc where to put the output. Unlike some other programs, fastqc will not create this directory if it does not already exist, which is why created it on the previous line.</p>
<p>Fastqc will generate a report for each FASTQ file in the output directory. We can look at each individually, or we can use multiqc to aggregate the results.</p>
<pre><code>cd fastqc_out

module load miniconda3/4.9.2 # we alread have swset gcc loaded
conda create -n multiqc
conda activate multiqc
conda install -c bioconda -c conda-forge multiqc

## The . in the next line command the present directory
##   this tells multiqc to look for output in the current directory
multiqc .</code></pre>
<p>Download the report from multiqc to your local machine.</p>
<pre><code>## DO THIS FROM A TERMINAL WINDOW *NOT* CONNECTED TO TETON
rsync -raz --progress &lt;yourusername&gt;@teton.uwyo.edu:&lt;your_path_to_/multiqc_report.html&gt; ~/Desktop</code></pre>
<p>This should download the file to your desktop. If you run this on a Windows machine, you may need to edit ~/Desktop to reflect the Windows location of the desktop, we have not run this on Windows.</p>
<p>An alternative to using <code>rsync</code> or <code>scp</code> for uploading or downloading files from Teton is to use a program like Filezilla or Cyberduck, which let you drag and drop files and view the Teton file system in a way that looks more like the typical file navigators on Mac or Windows.</p>
<p>If you double click this file, it should open up in the default web browser. Let’s take a look and go over some of these. From the start, in <code>General Statistics</code>, we can see that <code>SD_Field_0506</code> has very few sequences and <code>SD_Field_1453</code> doesn’t have very many either. If we remember from our iPyRad assemblies last week, both of these samples ended up with few loci overlapping other samples, and this is why. This is further evidence that these are failed samples and should be removed.</p>
<p>When we look at the <code>% Dups</code> in the <code>General Statistics</code>, we see what are at first alarmingly high numbers. However, this is because FastQC is designed for whole genome shotgun sequence data, where the genome is randomly sheared before sequencing. In that case, if two reads are identical, it likely means that they sequenced two PCR duplicates of the same starting molecule, instead of two different starting molecules. In RAD and other reduced-representation library methods, we explicitly enrich for specific loci, with each individual locus starting at the same position. This results in lots of identical reads, even if they start from different, sequencing from the same start site each time: we will end up with lots of copies of one or two (if heterozygous) identical sequences. This is roughly what is happening here.</p>
<p>The sequence quality histograms look good, with most qualities falling above 30, with slightly decreasing quality as the read progresses, as is common. If we had paired end reads, we would see that those reads would generally be slightly worse quality, with a faster and sharper quality drop off in quality through the read as a result of the order in which reads are sequenced and the chemistry of the sequencer.</p>
<p>In the <code>Per Base Sequence Content</code> section, we see that we have identical sequences at the beginning of each read. This is because we have the same restriction overhang at the start of each read by the nature of RADseq. This is another finding that would be cause for concern for whole genome data, but is expected with RADseq.</p>
<p>The GC content for sample <code>SD_Field_1453</code> looks very strange, further evidence that we should not use this sample moving forward. Farther down, we also see that <code>SD_Field_1453</code> also has a lot of over-represented sequences, most likely indicating some issue with library prep for this sample.</p>
<p>Looking at <code>Adapter Content</code> we can see that we have very little adapter present. his isn’t terribly surprising because these are 100 bp single-end reads on DNA fragments that were size selected to 415–515 bp.</p>
<p>Most of the rest of the checks either look good or are different ways of showing what we’ve discussed above.</p>
<p><br><br><br></p>
<h2 id="trimming-and-quality-filtering">2. Trimming and quality filtering</h2>
<p>Next up, we’ll use the tool <a href="http://www.usadellab.org/cms/?page=trimmomatic">Trimmomatic</a> to trim out any remaining adapters, as well trim reads for quality. This will remove portions of reads with low quality scores and entire reads if the entire read is of low quality or the length of the remaining read falls below the <code>MINLEN</code> threshold.</p>
<p>Let’s set this up as a job by creating a new slurm script to submit the job to Teton:</p>
<pre><code>cd .. # go to your demux_subset directory
# Let&#39;s delete SD_Field_1453.fastq and SD_Field_0506.fastq since we&#39;ve 
#    decided these are overall low quality samples and to speed this part up
rm  SD_Field_1453.fastq.gz SD_Field_0506.fastq.gz
nano trim.slurm # open and edit a new file</code></pre>
<p>Then add the following to the file, then close and save it</p>
<pre><code>#!/bin/sh

#SBATCH --job-name Trim
#SBATCH -A &lt;YOUR_ACCOUNT&gt;
#SBATCH -p teton
#SBATCH -t 1-00:00
#SBATCH --nodes=1
#SBATCH --cpus-per-task=6
#SBATCH --mem-per-cpu=5G
#SBATCH --mail-type=ALL
#SBATCH --mail-user=&lt;YOUR_EMAIL&gt;
#SBATCH -e trim_%A_%a.err
#SBATCH -o std_trim_%A_%a.out

module load swset/2018.05  gcc/7.3.0 trimmomatic/0.36

cd &lt;PATH_TO_YOUR/demux_subset&gt;

# create a loop to run trimmomatic on each of the samples in the 
#    current directory that ends with .fastq
for x in *fastq.gz; do 
    trimmomatic  SE -threads 6 $x trimmed_$x \
    ILLUMINACLIP:/project/inbre-train/2021_popgen_wkshp/data/TruSeq3-PE-2.fa.txt:2:30:10 \
    LEADING:3 TRAILING:3 MINLEN:36
done

# put the trimmed reads into a new directory
mkdir trimmed_reads
mv trimmed_*fastq.gz trimmed_reads</code></pre>
<p>Now we can submit this job using:</p>
<pre><code>sbatch trim.slurm</code></pre>
<p>We should rapidly start to see the trimmed file pop up and this should finish quickly.</p>
<p>Note that here we have trimmed adapters and low quality bases and reads, but our reads still have the restriction overhangs attached to them. If this was the main way that we were planning to handle our RAD data for analyses moving forward, we would want to further clean these up or use different trimming settings to remove these, but we won’t go through that here. For RAD data it’s typically simpler to use one of the existing pipelines like iPyRad, and for other types of data this won’t be necessary.</p>
<p>If you are dealing with paired end data, you will need to feed trimmomatic both your foward and reverse reads as input and specify more output files for forward and reverse reads, and reads of each that after trimming either do or or do not retain their matched read.</p>
<p><br><br><br></p>
<h2 id="post-trimming-quality-check">3. Post-trimming quality check</h2>
<p>Once the trimming has completed, we can run another round of FASTQC on these trimmed reads.</p>
<pre><code>salloc --account=&lt;YOUR_ACCOUNT&gt;  -t 0-0:30:00 --mem=4G --nodes=1 --ntasks-per-node=4
module load swset gcc fastqc/0.11.7 miniconda3/4.9.2 # load the modules necessary
cd trimmed_reads
mkdir fastqc_out
fastqc -t 4 *.fastq.gz -o fastqc_out

cd fastqc_out
conda activate multiqc
multiqc .</code></pre>
<p>Then we can download that new multiqc report and it should look better than the old one. Let’s rename it so it doesn’t overwrite, then download.</p>
<pre><code>mv multiqc_report.html multiqc_report_Trimmed.html

rsync -raz --progress &lt;yourusername&gt;@teton.uwyo.edu:&lt;your_path_to_/multiqc_report_Trimmed.html&gt; ~/Desktop</code></pre>
<p>This should all be familiar, and other than the few RAD-specific things like duplicates and some overhangs that are present, we shouldn’t see anything else of concern at this point.</p>
<p><br><br><br></p>
<h2 id="trimming-using-job-arrays">4. Trimming using job arrays</h2>
<p>When we trimmed our files above, we used a simple loop to trim all of the fastq files. This works well enough if you have relatively few, relatively small fastq files. However, if you have many, large files, using a loop that goes over them sequentially can take quite a while. My preferred solution to this is to use job arrays. This is an automated way of submitting replicate jobs as part of a single larger job, such that each “sub-job” can run in parallel, and all jobs can be submitted with a single slurm script and sbatch command.</p>
<p>The below will submit such a job array where each job will process a single fastq file when saved and run as a slurm script.</p>
<pre><code>#!/bin/bash

#SBATCH --job-name Trim
#SBATCH -A &lt;YOUR_ACCOUNT&gt;
#SBATCH -p teton
#SBATCH -t 1-00:00
#SBATCH --nodes=1
#SBATCH --cpus-per-task=10
#SBATCH --mem-per-cpu=24G
#SBATCH --mail-type=ALL
#SBATCH --mail-user=&lt;YOUR_EMAIL&gt;
#SBATCH -e err_trim_%A_%a.err
#SBATCH -o std_trim_%A_%a.out
#SBATCH --array=1-40


# load modules necessary to use conda on Teton
module load swset/2018.05  gcc/7.3.0 trimmomatic/0.36

# Set working directory to where the fastqs are
cd &lt;YOUR_DIRECTORY_TO_FASTQs_toTRIM&gt;


for x in *fastq.gz; do   # use a loop to find all the files that end in fastq.gz and assign them to a bash array
  trimfiles=(${trimfiles[@]} &quot;${x}&quot;)
done


## For whichever SLURM_ARRAY_TASK_ID index a job is in, get the sample 
##     name to simplify the trimmomatic call below
## here, I subtract 1 from the $SLURM_ARRAY_TASK_ID because bash indexing starts at zero
##   I think it&#39;s less confusing to subtract 1 here than to remember to do it when 
##   specifying the number of jobs for the array
sample=${trimfiles[($SLURM_ARRAY_TASK_ID-1)]}

# Run trimmomatic on each file

    trimmomatic  SE -threads 6 $sample trimmed_$sample \
    ILLUMINACLIP:/project/inbre-train/2021_popgen_wkshp/data/TruSeq3-PE-2.fa.txt:2:30:10 \
    LEADING:3 TRAILING:3 MINLEN:36

# put the trimmed reads into a directory
mkdir trimmed_reads
mv trimmed_*fastq.gz trimmed_reads</code></pre>
<p>Ensure that <code>#SBATCH --array=</code> is set to the correct number of jobs that you are running for any given job array. The script will effectively loop submit your job over the indices in that argument, replacing <code>$SLURM_ARRAY_TASK_ID</code> with the current index for each submission.</p>
</body>
</html>
